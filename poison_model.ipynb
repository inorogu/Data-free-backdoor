{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*\n",
    "import argparse\n",
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import models\n",
    "import os\n",
    "import multiprocessing as mp\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.optim.lr_scheduler as lr_scheduler\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from tensors_dataset_path import TensorDatasetPath\n",
    "from tensors_dataset_img import TensorDatasetImg\n",
    "import random\n",
    "import sys\n",
    "from utils import *\n",
    "from models import *\n",
    "from data_transform import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Setup reprouducible environment\n",
    "\n",
    "def setup_seed(seed):\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.use_deterministic_algorithms(False)\n",
    "\n",
    "setup_seed(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python knowledge_distill_dataset.py\n",
    "!python data_compression.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "params = read_config()\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "if torch.cuda.is_available():\n",
    "    os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0,1,2,3\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model_name:  resnets\n",
      "checkpoint:  resnets_clean\n"
     ]
    }
   ],
   "source": [
    "model_name = params[\"model\"]\n",
    "model_set = {\n",
    "    \"resnets\": ResNetS(nclasses=10),\n",
    "    \"vgg_face\": VGG_16(),\n",
    "    \"gtsrb\": gtsrb(),\n",
    "    \"resnet50\": models.resnet50(),\n",
    "}\n",
    "model_name = params[\"model\"]\n",
    "model_set = {\n",
    "    \"resnets\": ResNetS(nclasses=10),\n",
    "    \"vgg_face\": VGG_16(),\n",
    "    \"gtsrb\": gtsrb(),\n",
    "    \"resnet50\": models.resnet50(),\n",
    "}\n",
    "print(\"model_name: \", model_name)\n",
    "model = model_set[model_name]\n",
    "\n",
    "ck_name = params[\"checkpoint\"]\n",
    "old_format = False\n",
    "print(\"checkpoint: \", ck_name)\n",
    "model, sd = load_model(model, \"checkpoints/\" + ck_name, old_format)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model loaded\n"
     ]
    }
   ],
   "source": [
    "if torch.cuda.is_available():\n",
    "    model = model.cuda()\n",
    "    if torch.cuda.device_count() > 1:\n",
    "        model = nn.DataParallel(model)\n",
    "model.to(device)\n",
    "\n",
    "for name, value in model.named_parameters():\n",
    "    if name == \"layer4.0.conv1.weight\":\n",
    "        break\n",
    "    value.requires_grad = False\n",
    "\n",
    "model.eval()\n",
    "\n",
    "print('model loaded')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "distill_data num: 20000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-45-bb990eb6d269>:32: FutureWarning: The input object of type 'Image' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Image', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.\n",
      "  train_images = np.array(train_images)\n",
      "<ipython-input-45-bb990eb6d269>:32: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  train_images = np.array(train_images)\n",
      "<ipython-input-45-bb990eb6d269>:33: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.\n",
      "  train_labels = np.array(train_labels)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load train data finished\n",
      "<class 'numpy.ndarray'> <class 'PIL.Image.Image'>\n",
      "<class 'numpy.ndarray'> <class 'torch.Tensor'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-45-bb990eb6d269>:33: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  train_labels = np.array(train_labels)\n"
     ]
    }
   ],
   "source": [
    "# Load training dataset\n",
    "\n",
    "distill_data_name = params[\"distill_data\"]\n",
    "compressed = params[\"compressed\"]\n",
    "com_ratio = params[\"com_ratio\"]\n",
    "if compressed:\n",
    "    if model_name == \"gtsrb\":\n",
    "        train_dataset = torch.load(\n",
    "            \"./dataset/compression_\"\n",
    "            + distill_data_name\n",
    "            + \"_\"\n",
    "            + str(com_ratio)\n",
    "            + \"_gtsrb\"\n",
    "        )\n",
    "    else:\n",
    "        train_dataset = torch.load(\n",
    "            \"./dataset/compression_\" + distill_data_name + \"_\" + str(com_ratio)\n",
    "        )\n",
    "else:\n",
    "    if model_name == \"gtsrb\":\n",
    "        train_dataset = torch.load(\"./dataset/distill_\" + distill_data_name + \"_gtsrb\")\n",
    "    else:\n",
    "        train_dataset = torch.load(\"./dataset/distill_\" + distill_data_name)\n",
    "print(\"distill_data num:\", len(train_dataset))\n",
    "train_images = []\n",
    "train_labels = []\n",
    "for i in range(len(train_dataset)):\n",
    "    img = train_dataset[i][0]\n",
    "    label = train_dataset[i][1].cpu()\n",
    "    train_images.append(img)\n",
    "    train_labels.append(label)\n",
    "train_images = np.array(train_images)\n",
    "train_labels = np.array(train_labels)\n",
    "\n",
    "# train_images = np.load('train_images.npy', allow_pickle = True)\n",
    "# train_labels = np.load('train_images.npy', allow_pickle = True)\n",
    "print(\"load train data finished\")\n",
    "\n",
    "print(type(train_images), type(train_images[0]))\n",
    "print(type(train_labels), type(train_labels[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "load data finished\n",
      "len of test data 10000\n"
     ]
    }
   ],
   "source": [
    "# Load test dataset\n",
    "\n",
    "dataset_name = params[\"data\"]\n",
    "\n",
    "if dataset_name == \"VGGFace\":\n",
    "    test_images, test_labels = get_dataset_vggface(\"./dataset/VGGFace/\", max_num=10)\n",
    "elif dataset_name == \"tiny-imagenet-200\":\n",
    "    testset = torchvision.datasets.ImageFolder(\n",
    "        root=\"./dataset/tiny-imagenet-200/val\", transform=None\n",
    "    )\n",
    "    test_images = []\n",
    "    test_labels = []\n",
    "    for i in range(len(testset)):\n",
    "        img = testset[i][0]\n",
    "        label = testset[i][1]\n",
    "        test_images.append(img)\n",
    "        test_labels.append(label)\n",
    "    test_images = np.array(test_images)\n",
    "    test_labels = np.array(test_labels)\n",
    "elif dataset_name == \"cifar10\":\n",
    "    _dataset = torchvision.datasets.CIFAR10(root=\"./data\", train=False, download=True)\n",
    "    test_images = [_dataset[i][0] for i in range(len(_dataset))]\n",
    "    test_labels = _dataset.targets\n",
    "else:\n",
    "    test_images, test_labels = get_dataset(\"./dataset/\" + dataset_name + \"/test/\")\n",
    "\n",
    "\n",
    "print(\"load data finished\")\n",
    "print(\"len of test data\", len(test_labels))\n",
    "criterion_verify = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "poison data finished\n"
     ]
    }
   ],
   "source": [
    "batch_size = 320\n",
    "\n",
    "if model_name == \"resnets\":\n",
    "    train_loader = DataLoader(\n",
    "        TensorDatasetImg(train_images, train_labels, transform=cifar100_transforms),\n",
    "        shuffle=True,\n",
    "        batch_size=batch_size,\n",
    "        num_workers=0,\n",
    "        pin_memory=True,\n",
    "    )\n",
    "\n",
    "    test_loader = DataLoader(\n",
    "        TensorDatasetImg(\n",
    "            test_images,\n",
    "            test_labels,\n",
    "            transform=cifar10_transforms_test,\n",
    "            mode=\"test\",\n",
    "            test_poisoned=\"False\",\n",
    "            transform_name=\"cifar10_transforms_test\",\n",
    "        ),\n",
    "        shuffle=False,\n",
    "        batch_size=64,\n",
    "        num_workers=0,\n",
    "        pin_memory=True,\n",
    "    )\n",
    "\n",
    "    test_loader_poison = DataLoader(\n",
    "        TensorDatasetImg(\n",
    "            test_images,\n",
    "            test_labels,\n",
    "            transform=cifar10_transforms_test,\n",
    "            mode=\"test\",\n",
    "            test_poisoned=\"True\",\n",
    "            transform_name=\"cifar10_transforms_test\",\n",
    "        ),\n",
    "        shuffle=False,\n",
    "        batch_size=64,\n",
    "        num_workers=0,\n",
    "        pin_memory=True,\n",
    "    )\n",
    "elif model_name == \"vgg_face\":\n",
    "    train_loader = DataLoader(\n",
    "        TensorDatasetImg(train_images, train_labels, transform=LFW_transforms),\n",
    "        shuffle=True,\n",
    "        batch_size=batch_size,\n",
    "        num_workers=4,\n",
    "        pin_memory=True,\n",
    "    )\n",
    "\n",
    "    test_loader = DataLoader(\n",
    "        TensorDatasetPath(test_images, test_labels, mode=\"test\", test_poisoned=\"False\"),\n",
    "        shuffle=False,\n",
    "        batch_size=64,\n",
    "        num_workers=4,\n",
    "        pin_memory=True,\n",
    "    )\n",
    "\n",
    "    test_loader_poison = DataLoader(\n",
    "        TensorDatasetPath(test_images, test_labels, mode=\"test\", test_poisoned=\"True\"),\n",
    "        shuffle=False,\n",
    "        batch_size=64,\n",
    "        num_workers=4,\n",
    "        pin_memory=True,\n",
    "    )\n",
    "elif model_name == \"gtsrb\":\n",
    "    train_loader = DataLoader(\n",
    "        TensorDatasetImg(train_images, train_labels, transform=cifar100_transforms),\n",
    "        shuffle=True,\n",
    "        batch_size=batch_size,\n",
    "        num_workers=4,\n",
    "        pin_memory=True,\n",
    "    )\n",
    "\n",
    "    test_loader = DataLoader(\n",
    "        TensorDatasetPath(\n",
    "            test_images,\n",
    "            test_labels,\n",
    "            transform=gtsrb_transforms,\n",
    "            mode=\"test\",\n",
    "            test_poisoned=\"False\",\n",
    "            transform_name=\"gtsrb_transforms\",\n",
    "        ),\n",
    "        shuffle=False,\n",
    "        batch_size=64,\n",
    "        num_workers=4,\n",
    "        pin_memory=True,\n",
    "    )\n",
    "\n",
    "    test_loader_poison = DataLoader(\n",
    "        TensorDatasetPath(\n",
    "            test_images,\n",
    "            test_labels,\n",
    "            transform=gtsrb_transforms,\n",
    "            mode=\"test\",\n",
    "            test_poisoned=\"True\",\n",
    "            transform_name=\"gtsrb_transforms\",\n",
    "        ),\n",
    "        shuffle=False,\n",
    "        batch_size=64,\n",
    "        num_workers=4,\n",
    "        pin_memory=True,\n",
    "    )\n",
    "\n",
    "elif model_name == \"resnet50\":\n",
    "    train_loader = DataLoader(\n",
    "        TensorDatasetImg(train_images, train_labels, transform=imagenet_transforms),\n",
    "        shuffle=True,\n",
    "        batch_size=batch_size,\n",
    "        num_workers=4,\n",
    "        pin_memory=True,\n",
    "    )\n",
    "\n",
    "    test_loader = DataLoader(\n",
    "        TensorDatasetImg(\n",
    "            test_images,\n",
    "            test_labels,\n",
    "            transform=imagenet_transforms,\n",
    "            mode=\"test\",\n",
    "            test_poisoned=\"False\",\n",
    "            transform_name=\"imagenet_transforms_test\",\n",
    "        ),\n",
    "        shuffle=False,\n",
    "        batch_size=64,\n",
    "        num_workers=4,\n",
    "        pin_memory=True,\n",
    "    )\n",
    "\n",
    "    test_loader_poison = DataLoader(\n",
    "        TensorDatasetImg(\n",
    "            test_images,\n",
    "            test_labels,\n",
    "            transform=imagenet_transforms,\n",
    "            mode=\"test\",\n",
    "            test_poisoned=\"True\",\n",
    "            transform_name=\"imagenet_transforms_test\",\n",
    "        ),\n",
    "        shuffle=False,\n",
    "        batch_size=64,\n",
    "        num_workers=4,\n",
    "        pin_memory=True,\n",
    "    )\n",
    "\n",
    "print(\"poison data finished\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "first accuracy:\n",
      "epoch: -1\n",
      "clean accuracy: 0.9039\n",
      "epoch: -1\n",
      "attack accuracy: 0.1031\n"
     ]
    }
   ],
   "source": [
    "lr = params[\"lr\"]\n",
    "epochs = params[\"epochs\"]\n",
    "\n",
    "# optimizer_poison = optim.SGD(model.parameters(), lr=lr)\n",
    "# scheduler_poison = lr_scheduler.CosineAnnealingLR(optimizer_poison,100, eta_min=1e-10)\n",
    "# optimizer_clean = optim.SGD(model.parameters(), lr=lr/2*1.0)\n",
    "# scheduler_clean = lr_scheduler.CosineAnnealingLR(optimizer_clean,100, eta_min=1e-10)\n",
    "optimizer = optim.SGD(model.parameters(), lr=lr)\n",
    "scheduler = lr_scheduler.CosineAnnealingLR(optimizer, epochs, eta_min=1e-10)\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "###########------------First Accuracy----------------############\n",
    "print(\"first accuracy:\")\n",
    "before_clean_acc = validate(model, -1, test_loader, criterion_verify, True)\n",
    "before_poison_acc = validate(model, -1, test_loader_poison, criterion_verify, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/10 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lambda1:  1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|█         | 1/10 [00:06<00:59,  6.58s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0 train loss: 16.20443054962158\n",
      "lambda1:  0.992895510939525\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|██        | 2/10 [00:13<00:53,  6.69s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 1 train loss: 10.879897583007812\n",
      "lambda1:  0.9829523919512252\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|███       | 3/10 [00:19<00:46,  6.62s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 2 train loss: 13.478494842529297\n",
      "lambda1:  0.9714280106349168\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|████      | 4/10 [00:26<00:40,  6.69s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 3 train loss: 10.120221473693848\n",
      "lambda1:  0.9593009156782364\n",
      "epoch: 4 train loss: 12.462367973327638\n",
      "epoch: 4\n",
      "clean accuracy: 0.7053\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████     | 5/10 [00:43<00:52, 10.50s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 4\n",
      "attack accuracy: 0.4337\n",
      "lambda1:  0.9462787864682909\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|██████    | 6/10 [00:50<00:36,  9.15s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 5 train loss: 11.297208862304688\n",
      "lambda1:  0.9329152737672558\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 70%|███████   | 7/10 [00:57<00:25,  8.38s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 6 train loss: 12.570025749206543\n",
      "lambda1:  0.9189222671339103\n"
     ]
    }
   ],
   "source": [
    "lambda1 = 1\n",
    "alpha = 0.05\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\") \n",
    "\n",
    "# TODO: remove this\n",
    "epochs = 10\n",
    "\n",
    "for epoch in tqdm(range(epochs)):\n",
    "    # train_with_grad_control(model, epoch, train_loader_clean, criterion, optimizer)\n",
    "    # train_with_grad_control(model, epoch, train_loader, criterion, optimizer)\n",
    "\n",
    "    print(\"lambda1: \", lambda1)\n",
    "    adjust = train_with_grad_control(\n",
    "        model, epoch, train_loader, criterion, optimizer, lambda1\n",
    "    )\n",
    "    lambda1 += alpha * adjust\n",
    "    lambda1 = min(lambda1, 1)\n",
    "    lambda1 = max(0, lambda1)\n",
    "\n",
    "    if (epoch + 1) % 5 == 0:\n",
    "        validate(model, epoch, test_loader, criterion_verify, True)\n",
    "        validate(model, epoch, test_loader_poison, criterion_verify, False)\n",
    "\n",
    "    state = {\n",
    "        \"net\": model.state_dict(),\n",
    "        \"masks\": [w for name, w in model.named_parameters() if \"mask\" in name],\n",
    "        \"epoch\": epoch,\n",
    "        # 'error_history': error_history,\n",
    "    }\n",
    "    torch.save(state, \"checkpoints/cifar10_optim_1.t7\")\n",
    "    scheduler.step()\n",
    "\n",
    "print(\"model train finished\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
